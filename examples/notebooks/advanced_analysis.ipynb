{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Frame Extractor - Advanced Analysis\n",
    "\n",
    "This notebook demonstrates advanced analysis techniques using the YouTube Frame Extractor package. Building on the basics covered in the quickstart guide, we'll explore more sophisticated methods for extracting insights from video frames.\n",
    "\n",
    "## Advanced Topics Covered\n",
    "\n",
    "1. **Advanced VLM Analysis**: Fine-grained content matching and scoring\n",
    "2. **Object Detection**: Identifying and tracking objects across frames\n",
    "3. **Temporal Analysis**: Analyzing changes over time\n",
    "4. **Advanced Batch Processing**: Working with multiple videos efficiently\n",
    "5. **Data Visualization**: Creating insightful visualizations of your analysis\n",
    "6. **Custom Model Integration**: Using your own models for frame analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's set up our environment and import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add the parent directory to the path for importing the package\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Move up two directories from the current notebook location\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Verify we can import the package\n",
    "try:\n",
    "    from src.youtube_frame_extractor.extractors.browser import BrowserExtractor\n",
    "    from src.youtube_frame_extractor.extractors.download import DownloadExtractor\n",
    "    from src.youtube_frame_extractor.analysis.vlm import VLMAnalyzer\n",
    "    print(\"✅ Successfully imported YouTube Frame Extractor package\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing package: {str(e)}\")\n",
    "    print(\"Please make sure you're running this notebook from the examples/notebooks directory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import additional libraries for advanced analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('advanced_analysis')\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Create output directory for extracted frames\n",
    "output_dir = Path(\"./advanced_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Output will be saved to: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "\n",
    "Let's define some advanced utility functions for display and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def display_frames(frames, max_frames=6, figsize=(15, 10), title=\"Extracted Frames\", annotations=None):\n",
    "    \"\"\"Display a grid of extracted frames with optional annotations.\n",
    "    \n",
    "    Args:\n",
    "        frames: List of frame dictionaries\n",
    "        max_frames: Maximum number of frames to display\n",
    "        figsize: Figure size as (width, height)\n",
    "        title: Title for the overall figure\n",
    "        annotations: Optional dict mapping frame indices to annotation text\n",
    "    \"\"\"\n",
    "    num_frames = min(max_frames, len(frames))\n",
    "    if num_frames == 0:\n",
    "        print(\"No frames to display\")\n",
    "        return\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    cols = min(3, num_frames)\n",
    "    rows = (num_frames + cols - 1) // cols\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        \n",
    "        # Get the frame image\n",
    "        if 'frame' in frames[i] and frames[i]['frame'] is not None:\n",
    "            img = frames[i]['frame']\n",
    "        elif 'path' in frames[i] and os.path.exists(frames[i]['path']):\n",
    "            img = Image.open(frames[i]['path'])\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, \"Image not available\", ha='center', va='center')\n",
    "            plt.axis('off')\n",
    "            continue\n",
    "        \n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        \n",
    "        subtitle = f\"Frame {i+1}\"\n",
    "        if 'time' in frames[i]:\n",
    "            subtitle += f\" | Time: {frames[i]['time']:.2f}s\"\n",
    "        if 'similarity' in frames[i]:\n",
    "            subtitle += f\" | Score: {frames[i]['similarity']:.2f}\"\n",
    "        \n",
    "        if annotations and i in annotations:\n",
    "            subtitle += f\"\\n{annotations[i]}\"\n",
    "            \n",
    "        plt.title(subtitle)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "def display_video_info(video_id):\n",
    "    \"\"\"Display YouTube video embed and basic info.\"\"\"\n",
    "    embed_html = f\"\"\"\n",
    "    <div style=\\\"width:560px;\\\">\\n\",\n",
    "        <h3>YouTube Video: {video_id}</h3>\\n\",\n",
    "        <iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/{video_id}\\\" \\n\",\n",
    "                frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; clipboard-write; encrypted-media; \\n\",\n",
    "                gyroscope; picture-in-picture\\\" allowfullscreen>\\n\",\n",
    "        </iframe>\\n\",\n",
    "    </div>\\n\",\n",
    "    \"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(embed_html))\n",
    "\n",
    "def plot_similarity_timeline(frames, title=\"Similarity Scores Over Time\", figsize=(12, 6)):\n",
    "    \"\"\"Plot similarity scores over time from frame data.\n",
    "    \n",
    "    Args:\n",
    "        frames: List of frame dictionaries with 'time' and 'similarity' keys\n",
    "        title: Plot title\n",
    "        figsize: Figure size as (width, height)\n",
    "    \"\"\"\n",
    "    times = [frame.get('time', i) for i, frame in enumerate(frames)]\n",
    "    scores = [frame.get('similarity', 0) for frame in frames]\n",
    "    \n",
    "    if not times or not scores:\n",
    "        print(\"No data available for timeline plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(times, scores, '-o', linewidth=2, markersize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "    plt.ylabel(\"Similarity Score\", fontsize=12)\n",
    "    \n",
    "    thresholds = {frame.get('threshold', None) for frame in frames if 'threshold' in frame}\n",
    "    if len(thresholds) == 1 and None not in thresholds:\n",
    "        threshold = thresholds.pop()\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', alpha=0.7, label=f\"Threshold ({threshold})\")\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(figsize[0], figsize[1]//2))\n",
    "    heatmap_data = df = None\n",
    "    try:\n",
    "        heatmap_data = pd.DataFrame({\n",
    "            query: [frame.get('similarity', 0) for frame in frames] for query in [\"similarity\"]\n",
    "        })\n",
    "        sns.heatmap(heatmap_data, cmap=\"YlGnBu\", cbar_kws={'label': 'Similarity Score'})\n",
    "        plt.xlabel(\"Frame Index\")\n",
    "        plt.ylabel(\"Query\")\n",
    "        plt.title(\"Similarity Scores Across All Frames and Queries\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting heatmap: {str(e)}\")\n",
    "\n",
    "def apply_text_overlay(image, text, position=(10, 10), font_size=20, color=(255, 255, 255), bg_color=(0, 0, 0, 128)):\n",
    "    \"\"\"Apply text overlay to an image.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        text: Text to overlay\n",
    "        position: (x, y) position for text\n",
    "        font_size: Text font size\n",
    "        color: Text color as RGB tuple\n",
    "        bg_color: Background color as RGBA tuple\n",
    "    Returns:\n",
    "        PIL Image with text overlay\n",
    "    \"\"\"\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "    annotated_image = image.copy()\n",
    "    try:\n",
    "        from PIL import ImageFont\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    draw = ImageDraw.Draw(annotated_image, 'RGBA')\n",
    "    text_width, text_height = draw.textsize(text, font=font)\n",
    "    draw.rectangle([\n",
    "        position[0], position[1], position[0] + text_width + 10, position[1] + text_height + 10\n",
    "    ], fill=bg_color)\n",
    "    draw.text((position[0] + 5, position[1] + 5), text, font=font, fill=color)\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced VLM Analysis\n",
    "\n",
    "Now let's explore more sophisticated VLM-based frame analysis, including:\n",
    "- Multi-query analysis\n",
    "- Concept comparison\n",
    "- Fine-grained visual attribute detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a function for multi-query VLM analysis\n",
    "def analyze_frames_with_multiple_queries(frames, queries, vlm_analyzer=None):\n",
    "    \"\"\"Analyze frames with multiple queries and return similarity scores for each.\n",
    "    \n",
    "    Args:\n",
    "        frames: List of frame dictionaries\n",
    "        queries: List of query strings\n",
    "        vlm_analyzer: VLMAnalyzer instance (will be created if None)\n",
    "    Returns:\n",
    "        DataFrame with similarity scores for each query and frame\n",
    "    \"\"\"\n",
    "    if not frames or not queries:\n",
    "        print(\"No frames or queries provided\")\n",
    "        return None\n",
    "    if vlm_analyzer is None:\n",
    "        try:\n",
    "            vlm_analyzer = VLMAnalyzer(model_name=\"openai/clip-vit-base-patch16\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing VLM analyzer: {str(e)}\")\n",
    "            return None\n",
    "    results = { 'frame_index': [], 'time': [] }\n",
    "    for query in queries:\n",
    "        results[f\"score_{query.replace(' ', '_')}\"] = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        results['frame_index'].append(i)\n",
    "        results['time'].append(frame.get('time', i))\n",
    "        if 'frame' in frame and frame['frame'] is not None:\n",
    "            image = frame['frame']\n",
    "        elif 'path' in frame and os.path.exists(frame['path']):\n",
    "            image = Image.open(frame['path'])\n",
    "        else:\n",
    "            for query in queries:\n",
    "                results[f\"score_{query.replace(' ', '_')}\"].append(0.0)\n",
    "            continue\n",
    "        for query in queries:\n",
    "            try:\n",
    "                similarity = vlm_analyzer.calculate_similarity(image, query)\n",
    "                results[f\"score_{query.replace(' ', '_')}\"].append(float(similarity))\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating similarity for query '{query}': {str(e)}\")\n",
    "                results[f\"score_{query.replace(' ', '_')}\"].append(0.0)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def visualize_multi_query_results(df, queries, figsize=(12, 8)):\n",
    "    \"\"\"Visualize multi-query analysis results.\n",
    "    Args:\n",
    "        df: DataFrame with similarity scores\n",
    "        queries: List of original query strings\n",
    "        figsize: Figure size as (width, height)\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to visualize\")\n",
    "        return\n",
    "    plt.figure(figsize=figsize)\n",
    "    for query in queries:\n",
    "        col_name = f\"score_{query.replace(' ', '_')}\"\n",
    "        if col_name in df.columns:\n",
    "            plt.plot(df['time'], df[col_name], '-o', label=query, linewidth=2, markersize=6)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title(\"Multi-Query Analysis Results\", fontsize=14)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "    plt.ylabel(\"Similarity Score\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(figsize[0], figsize[1]//2))\n",
    "    heatmap_data = df[[f\"score_{query.replace(' ', '_')}\" for query in queries]].copy()\n",
    "    heatmap_data.columns = queries\n",
    "    sns.heatmap(heatmap_data.T, cmap=\"YlGnBu\", cbar_kws={'label': 'Similarity Score'})\n",
    "    plt.xlabel(\"Frame Index\")\n",
    "    plt.ylabel(\"Query\")\n",
    "    plt.title(\"Similarity Scores Across All Frames and Queries\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Summary Statistics:\")\n",
    "    for query in queries:\n",
    "        col_name = f\"score_{query.replace(' ', '_')}\"\n",
    "        if col_name in df.columns:\n",
    "            max_score = df[col_name].max()\n",
    "            max_frame = df.loc[df[col_name].idxmax(), 'frame_index']\n",
    "            max_time = df.loc[df[col_name].idxmax(), 'time']\n",
    "            print(f\"- '{query}': Max score {max_score:.3f} at frame {int(max_frame)} (time: {max_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up our video for analysis\n",
    "# Using Nature documentary as an example (generic example)\n",
    "video_id = \"nLrrOcXX2kw\"  # Example nature documentary\n",
    "\n",
    "# Display the video for reference\n",
    "display_video_info(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract frames for analysis\n",
    "try:\n",
    "    download_extractor = DownloadExtractor(output_dir=str(output_dir / \"multi_query\"))\n",
    "    frames = download_extractor.extract_frames(\n",
    "        video_id=video_id,\n",
    "        frame_rate=0.1,  # One frame every 10 seconds\n",
    "        max_frames=20    # Up to 20 frames\n",
    "    )\n",
    "    print(f\"Successfully extracted {len(frames)} frames for multi-query analysis\")\n",
    "    display_frames(frames[:6], title=\"Sample Frames for Analysis\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting frames: {str(e)}\")\n",
    "    frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define multiple queries for analysis\n",
    "nature_queries = [\n",
    "    \"forest landscape\", \n",
    "    \"wild animals\", \n",
    "    \"underwater scene\",\n",
    "    \"mountain vista\",\n",
    "    \"birds flying\"\n",
    "]\n",
    "\n",
    "# Initialize VLM analyzer and perform multi-query analysis\n",
    "try:\n",
    "    vlm_analyzer = VLMAnalyzer(model_name=\"openai/clip-vit-base-patch16\")\n",
    "    print(\"✅ VLM analyzer initialized successfully\")\n",
    "    print(\"\\nAnalyzing frames with multiple queries...\")\n",
    "    multi_query_results = analyze_frames_with_multiple_queries(\n",
    "        frames=frames,\n",
    "        queries=nature_queries,\n",
    "        vlm_analyzer=vlm_analyzer\n",
    "    )\n",
    "    print(\"\\nMulti-query analysis complete!\")\n",
    "    visualize_multi_query_results(multi_query_results, nature_queries)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in multi-query analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Frame Montages Based on Content\n",
    "\n",
    "Let's demonstrate how to create content-specific montages based on VLM analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_content_montage(frames, query, vlm_analyzer, threshold=0.3, max_frames=6, cols=3, frame_size=(320, 180)):\n",
    "    \"\"\"Create a montage of frames that match a specific content query.\n",
    "    Args:\n",
    "        frames: List of frame dictionaries\n",
    "        query: Content query to match\n",
    "        vlm_analyzer: VLM analyzer instance\n",
    "        threshold: Minimum similarity score to include frame\n",
    "        max_frames: Maximum number of frames to include\n",
    "        cols: Number of columns in the montage\n",
    "        frame_size: Size to resize each frame to (width, height)\n",
    "    Returns:\n",
    "        PIL Image containing the montage\n",
    "    \"\"\"\n",
    "    if not frames:\n",
    "        print(\"No frames provided for montage creation\")\n",
    "        return None\n",
    "    scored_frames = []\n",
    "    for frame in frames:\n",
    "        if 'frame' in frame and frame['frame'] is not None:\n",
    "            image = frame['frame']\n",
    "        elif 'path' in frame and os.path.exists(frame['path']):\n",
    "            image = Image.open(frame['path'])\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            similarity = vlm_analyzer.calculate_similarity(image, query)\n",
    "            scored_frames.append({\n",
    "                'frame': image,\n",
    "                'similarity': float(similarity),\n",
    "                'time': frame.get('time', 0)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity: {str(e)}\")\n",
    "    scored_frames.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    matching_frames = [f for f in scored_frames if f['similarity'] >= threshold]\n",
    "    if not matching_frames:\n",
    "        print(f\"No frames matched the query '{query}' with threshold {threshold}\")\n",
    "        return None\n",
    "    matching_frames = matching_frames[:max_frames]\n",
    "    num_frames = len(matching_frames)\n",
    "    rows = (num_frames + cols - 1) // cols\n",
    "    montage_width = cols * frame_size[0]\n",
    "    montage_height = rows * frame_size[1]\n",
    "    montage = Image.new('RGB', (montage_width, montage_height))\n",
    "    for i, frame_data in enumerate(matching_frames):\n",
    "        img = frame_data['frame'].copy()\n",
    "        img = img.resize(frame_size, Image.LANCZOS)\n",
    "        text = f\"Score: {frame_data['similarity']:.2f} | Time: {frame_data['time']:.1f}s\"\n",
    "        img = apply_text_overlay(img, text)\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        x = col * frame_size[0]\n",
    "        y = row * frame_size[1]\n",
    "        montage.paste(img, (x, y))\n",
    "    return montage\n",
    "\n",
    "# Create and display montages for each query\n",
    "if 'vlm_analyzer' in locals() and frames:\n",
    "    for query in nature_queries:\n",
    "        try:\n",
    "            print(f\"Creating montage for query: '{query}'\")\n",
    "            montage = create_content_montage(\n",
    "                frames=frames,\n",
    "                query=query,\n",
    "                vlm_analyzer=vlm_analyzer,\n",
    "                threshold=0.25,\n",
    "                max_frames=6\n",
    "            )\n",
    "            if montage is not None:\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.imshow(montage)\n",
    "                plt.title(f\"Content Montage: '{query}'\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                montage_path = output_dir / f\"montage_{query.replace(' ', '_')}.jpg\"\n",
    "                montage.save(montage_path)\n",
    "                print(f\"Montage saved to {montage_path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating montage for '{query}': {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Detection and Tracking\n",
    "\n",
    "Let's implement advanced object detection and tracking across frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_objects(image, confidence_threshold=0.5):\n",
    "    \"\"\"Detect objects in an image using a pre-trained model.\n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        confidence_threshold: Minimum confidence score for detections\n",
    "    Returns:\n",
    "        List of detected objects with class, confidence, and bounding box\n",
    "    \"\"\"\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "    try:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading object detection model: {str(e)}\")\n",
    "        return []\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(image)\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image_tensor])\n",
    "    detections = []\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    COCO_CLASSES = [\n",
    "        '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "        'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "        'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "        'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "    ]\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(image)\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image_tensor])\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score >= confidence_threshold:\n",
    "            class_name = COCO_CLASSES[label]\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            detections.append({\n",
    "                'class': class_name,\n",
    "                'confidence': float(score),\n",
    "                'box': box.astype(int).tolist()\n",
    "            })\n",
    "    return detections\n",
    "\n",
    "def visualize_detections(image, detections):\n",
    "    \"\"\"Draw bounding boxes and labels for detected objects.\n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        detections: List of detection dictionaries\n",
    "    Returns:\n",
    "        PIL Image with detection visualizations\n",
    "    \"\"\"\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "    vis_image = image.copy()\n",
    "    draw = ImageDraw.Draw(vis_image)\n",
    "    colors = [\n",
    "        (255, 0, 0),    \n",
    "        (0, 255, 0),    \n",
    "        (0, 0, 255),    \n",
    "        (255, 255, 0),  \n",
    "        (255, 0, 255),  \n",
    "        (0, 255, 255)   \n",
    "    ]\n",
    "    for i, det in enumerate(detections):\n",
    "        box = det['box']\n",
    "        class_name = det['class']\n",
    "        confidence = det['confidence']\n",
    "        color_idx = hash(class_name) % len(colors)\n",
    "        color = colors[color_idx]\n",
    "        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=3)\n",
    "        label_text = f\"{class_name} {confidence:.2f}\"\n",
    "        text_width, text_height = draw.textsize(label_text)\n",
    "        draw.rectangle([\n",
    "            (box[0], box[1] - text_height - 4), (box[0] + text_width + 4, box[1])\n",
    "        ], fill=color)\n",
    "        draw.text((box[0] + 2, box[1] - text_height - 2), label_text, fill=(255, 255, 255))\n",
    "    return vis_image\n",
    "\n",
    "def detect_and_track_objects_across_frames(frames, confidence_threshold=0.5):\n",
    "    \"\"\"Detect objects across multiple frames and track their occurrences.\n",
    "    Args:\n",
    "        frames: List of frame dictionaries\n",
    "        confidence_threshold: Minimum confidence score for detections\n",
    "    Returns:\n",
    "        Tuple of (processed frames, object occurrence data)\n",
    "    \"\"\"\n",
    "    if not frames:\n",
    "        return [], {}\n",
    "    processed_frames = []\n",
    "    object_occurrences = {}\n",
    "    for i, frame in enumerate(frames):\n",
    "        if 'frame' in frame and frame['frame'] is not None:\n",
    "            image = frame['frame']\n",
    "        elif 'path' in frame and os.path.exists(frame['path']):\n",
    "            image = Image.open(frame['path'])\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            detections = detect_objects(image, confidence_threshold)\n",
    "            for det in detections:\n",
    "                class_name = det['class']\n",
    "                if class_name not in object_occurrences:\n",
    "                    object_occurrences[class_name] = {'count': 0, 'frames': [], 'confidences': []}\n",
    "                object_occurrences[class_name]['count'] += 1\n",
    "                object_occurrences[class_name]['frames'].append(i)\n",
    "                object_occurrences[class_name]['confidences'].append(det['confidence'])\n",
    "            vis_image = visualize_detections(image, detections)\n",
    "            processed_frame = frame.copy()\n",
    "            processed_frame['frame'] = vis_image\n",
    "            processed_frame['detections'] = detections\n",
    "            processed_frame['object_count'] = len(detections)\n",
    "            processed_frames.append(processed_frame)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {i}: {str(e)}\")\n",
    "            processed_frames.append(frame)\n",
    "    return processed_frames, object_occurrences\n",
    "\n",
    "def visualize_object_occurrences(object_occurrences, min_count=2, figsize=(12, 8)):\n",
    "    \"\"\"Visualize object occurrences across frames.\n",
    "    Args:\n",
    "        object_occurrences: Dictionary of object occurrence data\n",
    "        min_count: Minimum count to include in visualization\n",
    "        figsize: Figure size as (width, height)\n",
    "    \"\"\"\n",
    "    if not object_occurrences:\n",
    "        print(\"No object occurrence data to visualize\")\n",
    "        return\n",
    "    filtered_objects = {k: v for k, v in object_occurrences.items() if v['count'] >= min_count}\n",
    "    if not filtered_objects:\n",
    "        print(f\"No objects detected in at least {min_count} frames\")\n",
    "        return\n",
    "    sorted_objects = sorted(filtered_objects.items(), key=lambda x: x[1]['count'], reverse=True)\n",
    "    objects = [x[0] for x in sorted_objects]\n",
    "    counts = [x[1]['count'] for x in sorted_objects]\n",
    "    plt.figure(figsize=figsize)\n",
    "    bars = plt.bar(objects, counts, color='steelblue')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(\"Object Detection Frequency\", fontsize=14)\n",
    "    plt.xlabel(\"Object Class\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Occurrences\", fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{int(height)}', ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run object detection on our frames\n",
    "if 'frames' in locals() and frames:\n",
    "    try:\n",
    "        print(\"Detecting objects across frames...\")\n",
    "        processed_frames, object_occurrences = detect_and_track_objects_across_frames(\n",
    "            frames=frames[:8],  # Limit to 8 frames for demo\n",
    "            confidence_threshold=0.4\n",
    "        )\n",
    "        print(f\"Detected objects in {len(processed_frames)} frames\")\n",
    "        display_frames(\n",
    "            processed_frames, \n",
    "            title=\"Frames with Object Detection\",\n",
    "            annotations={i: f\"Objects: {frame.get('object_count', 0)}\" for i, frame in enumerate(processed_frames)}\n",
    "        )\n",
    "        visualize_object_occurrences(object_occurrences)\n",
    "        print(\"\\nDetailed Object Occurrence Data:\")\n",
    "        for obj, data in sorted(object_occurrences.items(), key=lambda x: x[1]['count'], reverse=True):\n",
    "            if data['count'] >= 2:\n",
    "                avg_conf = sum(data['confidences']) / len(data['confidences'])\n",
    "                print(f\"- {obj}: {data['count']} occurrences, avg confidence: {avg_conf:.3f}\")\n",
    "                print(f\"  Appears in frames: {', '.join(str(f) for f in data['frames'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in object detection: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis\n",
    "\n",
    "Let's analyze how content changes over time in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_frame_differences(frames, method='mse'):\n",
    "    \"\"\"Calculate differences between consecutive frames.\n",
    "    Args:\n",
    "        frames: List of frame dictionaries\n",
    "        method: Difference calculation method ('mse', 'ssim', or 'histogram')\n",
    "    Returns:\n",
    "        List of dictionaries with frame differences\n",
    "    \"\"\"\n",
    "    if not frames or len(frames) < 2:\n",
    "        print(\"Not enough frames for difference calculation\")\n",
    "        return []\n",
    "    differences = []\n",
    "    for i in range(1, len(frames)):\n",
    "        prev_frame = None\n",
    "        if 'frame' in frames[i-1] and frames[i-1]['frame'] is not None:\n",
    "            prev_frame = frames[i-1]['frame']\n",
    "        elif 'path' in frames[i-1] and os.path.exists(frames[i-1]['path']):\n",
    "            prev_frame = Image.open(frames[i-1]['path'])\n",
    "        curr_frame = None\n",
    "        if 'frame' in frames[i] and frames[i]['frame'] is not None:\n",
    "            curr_frame = frames[i]['frame']\n",
    "        elif 'path' in frames[i] and os.path.exists(frames[i]['path']):\n",
    "            curr_frame = Image.open(frames[i]['path'])\n",
    "        if prev_frame is None or curr_frame is None:\n",
    "            continue\n",
    "        if isinstance(prev_frame, Image.Image):\n",
    "            prev_frame = np.array(prev_frame)\n",
    "        if isinstance(curr_frame, Image.Image):\n",
    "            curr_frame = np.array(curr_frame)\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n",
    "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n",
    "        diff_value = 0\n",
    "        diff_image = None\n",
    "        if method == 'mse':\n",
    "            diff = np.square(np.subtract(prev_gray, curr_gray)).mean()\n",
    "            diff_value = float(diff)\n",
    "            diff_image = cv2.absdiff(prev_gray, curr_gray)\n",
    "            diff_image = cv2.cvtColor(diff_image, cv2.COLOR_GRAY2RGB)\n",
    "        elif method == 'ssim':\n",
    "            from skimage.metrics import structural_similarity as ssim\n",
    "            score, diff_image = ssim(prev_gray, curr_gray, full=True)\n",
    "            diff_value = 1.0 - score\n",
    "            diff_image = (diff_image * 255).astype(\"uint8\")\n",
    "            diff_image = cv2.cvtColor(diff_image, cv2.COLOR_GRAY2RGB)\n",
    "        elif method == 'histogram':\n",
    "            prev_hist = cv2.calcHist([prev_gray], [0], None, [256], [0, 256])\n",
    "            curr_hist = cv2.calcHist([curr_gray], [0], None, [256], [0, 256])\n",
    "            cv2.normalize(prev_hist, prev_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "            cv2.normalize(curr_hist, curr_hist, 0, 1, cv2.NORM_MINMAX)\n",
    "            diff_value = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "            diff_image = cv2.absdiff(prev_frame, curr_frame)\n",
    "        difference = {\n",
    "            'prev_index': i-1,\n",
    "            'curr_index': i,\n",
    "            'prev_time': frames[i-1].get('time', i-1),\n",
    "            'curr_time': frames[i].get('time', i),\n",
    "            'difference': diff_value,\n",
    "            'diff_image': Image.fromarray(diff_image) if diff_image is not None else None\n",
    "        }\n",
    "        differences.append(difference)\n",
    "    return differences\n",
    "\n",
    "def detect_scene_changes(differences, threshold=0.5):\n",
    "    \"\"\"Detect potential scene changes based on frame differences.\n",
    "    Args:\n",
    "        differences: List of frame difference dictionaries\n",
    "        threshold: Threshold for considering a difference a scene change\n",
    "    Returns:\n",
    "        List of indices where scene changes occur\n",
    "    \"\"\"\n",
    "    if not differences:\n",
    "        return []\n",
    "    diff_values = [d['difference'] for d in differences]\n",
    "    if threshold is None:\n",
    "        mean_diff = np.mean(diff_values)\n",
    "        std_diff = np.std(diff_values)\n",
    "        threshold = mean_diff + 2 * std_diff\n",
    "    scene_changes = []\n",
    "    for i, diff in enumerate(differences):\n",
    "        if diff['difference'] > threshold:\n",
    "            scene_changes.append(i)\n",
    "    return scene_changes\n",
    "\n",
    "def visualize_frame_differences(differences, scene_changes=None, figsize=(12, 6)):\n",
    "    \"\"\"Visualize frame differences and scene changes.\n",
    "    Args:\n",
    "        differences: List of frame difference dictionaries\n",
    "        scene_changes: List of indices where scene changes occur\n",
    "        figsize: Figure size as (width, height)\n",
    "    \"\"\"\n",
    "    if not differences:\n",
    "        print(\"No differences to visualize\")\n",
    "        return\n",
    "    times = [(d['prev_time'] + d['curr_time']) / 2 for d in differences]\n",
    "    diff_values = [d['difference'] for d in differences]\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(times, diff_values, '-o', linewidth=2, markersize=6)\n",
    "    if scene_changes:\n",
    "        for sc in scene_changes:\n",
    "            if 0 <= sc < len(differences):\n",
    "                plt.axvline(x=times[sc], color='r', linestyle='--', alpha=0.7)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title(\"Frame Differences Over Time\", fontsize=14)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "    plt.ylabel(\"Difference Value\", fontsize=12)\n",
    "    if scene_changes:\n",
    "        plt.legend([\"Frame Difference\", \"Scene Change\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if scene_changes and any(0 <= sc < len(differences) for sc in scene_changes):\n",
    "        valid_changes = [sc for sc in scene_changes if 0 <= sc < len(differences)][:3]\n",
    "        fig, axes = plt.subplots(len(valid_changes), 3, figsize=(15, 5*len(valid_changes)))\n",
    "        if len(valid_changes) == 1:\n",
    "            axes = [axes]\n",
    "        for i, sc in enumerate(valid_changes):\n",
    "            diff = differences[sc]\n",
    "            prev_frame = None\n",
    "            if 'frame' in frames[diff['prev_index']] and frames[diff['prev_index']]['frame'] is not None:\n",
    "                prev_frame = frames[diff['prev_index']]['frame']\n",
    "            elif 'path' in frames[diff['prev_index']] and os.path.exists(frames[diff['prev_index']]['path']):\n",
    "                prev_frame = Image.open(frames[diff['prev_index']]['path'])\n",
    "            curr_frame = None\n",
    "            if 'frame' in frames[diff['curr_index']] and frames[diff['curr_index']]['frame'] is not None:\n",
    "                curr_frame = frames[diff['curr_index']]['frame']\n",
    "            elif 'path' in frames[diff['curr_index']] and os.path.exists(frames[diff['curr_index']]['path']):\n",
    "                curr_frame = Image.open(frames[diff['curr_index']]['path'])\n",
    "            if prev_frame is not None:\n",
    "                axes[i][0].imshow(np.array(prev_frame))\n",
    "                axes[i][0].set_title(f\"Before (time: {diff['prev_time']:.2f}s)\")\n",
    "                axes[i][0].axis('off')\n",
    "            if curr_frame is not None:\n",
    "                axes[i][1].imshow(np.array(curr_frame))\n",
    "                axes[i][1].set_title(f\"After (time: {diff['curr_time']:.2f}s)\")\n",
    "                axes[i][1].axis('off')\n",
    "            if diff['diff_image'] is not None:\n",
    "                axes[i][2].imshow(np.array(diff['diff_image']))\n",
    "                axes[i][2].set_title(f\"Difference (value: {diff['difference']:.4f})\")\n",
    "                axes[i][2].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(\"Scene Change Visualization\", fontsize=16, y=1.02)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze frame differences and detect scene changes\n",
    "if 'frames' in locals() and len(frames) >= 2:\n",
    "    try:\n",
    "        print(\"Calculating frame differences...\")\n",
    "        differences = calculate_frame_differences(frames, method='mse')\n",
    "        print(f\"Calculated {len(differences)} frame differences\")\n",
    "        diff_values = [d['difference'] for d in differences]\n",
    "        mean_diff = np.mean(diff_values)\n",
    "        std_diff = np.std(diff_values)\n",
    "        threshold = mean_diff + 1.5 * std_diff\n",
    "        print(f\"Using threshold {threshold:.4f} for scene change detection\")\n",
    "        scene_changes = detect_scene_changes(differences, threshold=threshold)\n",
    "        print(f\"Detected {len(scene_changes)} potential scene changes\")\n",
    "        visualize_frame_differences(differences, scene_changes)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing frame differences: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Batch Processing\n",
    "\n",
    "Now let's demonstrate advanced batch processing with progress tracking and parallel execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_video(video_id, query, output_dir, frame_rate=0.1, max_frames=10):\n",
    "    \"\"\"Process a single video with VLM-based frame extraction and analysis.\n",
    "    Args:\n",
    "        video_id: YouTube video ID\n",
    "        query: Natural language query for content matching\n",
    "        output_dir: Directory to save output\n",
    "        frame_rate: Frames per second to extract\n",
    "        max_frames: Maximum number of frames to extract\n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        video_dir = os.path.join(output_dir, video_id)\n",
    "        os.makedirs(video_dir, exist_ok=True)\n",
    "        extractor = DownloadExtractor(output_dir=video_dir)\n",
    "        frames = extractor.extract_frames(\n",
    "            video_id=video_id,\n",
    "            frame_rate=frame_rate,\n",
    "            max_frames=max_frames\n",
    "        )\n",
    "        vlm_analyzer = VLMAnalyzer(model_name=\"openai/clip-vit-base-patch16\")\n",
    "        scored_frames = []\n",
    "        for frame in frames:\n",
    "            if 'frame' in frame and frame['frame'] is not None:\n",
    "                image = frame['frame']\n",
    "            elif 'path' in frame and os.path.exists(frame['path']):\n",
    "                image = Image.open(frame['path'])\n",
    "            else:\n",
    "                continue\n",
    "            similarity = vlm_analyzer.calculate_similarity(image, query)\n",
    "            scored_frame = frame.copy()\n",
    "            scored_frame['similarity'] = float(similarity)\n",
    "            scored_frame['query'] = query\n",
    "            scored_frames.append(scored_frame)\n",
    "        scored_frames.sort(key=lambda x: x.get('similarity', 0), reverse=True)\n",
    "        top_frames = scored_frames[:min(6, len(scored_frames))]\n",
    "        montage = create_content_montage(\n",
    "            frames=frames,\n",
    "            query=query,\n",
    "            vlm_analyzer=vlm_analyzer,\n",
    "            threshold=0.2,\n",
    "            max_frames=6\n",
    "        )\n",
    "        montage_path = os.path.join(video_dir, f\"{video_id}_{query.replace(' ', '_')}_montage.jpg\")\n",
    "        if montage is not None:\n",
    "            montage.save(montage_path)\n",
    "        metadata = {\n",
    "            'video_id': video_id,\n",
    "            'query': query,\n",
    "            'frame_count': len(frames),\n",
    "            'top_score': scored_frames[0]['similarity'] if scored_frames else 0,\n",
    "            'average_score': sum(f['similarity'] for f in scored_frames) / len(scored_frames) if scored_frames else 0,\n",
    "            'montage_path': montage_path if montage is not None else None\n",
    "        }\n",
    "        metadata_path = os.path.join(video_dir, f\"{video_id}_{query.replace(' ', '_')}_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        return {\n",
    "            'video_id': video_id,\n",
    "            'success': True,\n",
    "            'frames': scored_frames,\n",
    "            'metadata': metadata,\n",
    "            'montage_path': montage_path if montage is not None else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"Error processing video {video_id}: {error_message}\")\n",
    "        return {\n",
    "            'video_id': video_id,\n",
    "            'success': False,\n",
    "            'error': error_message,\n",
    "            'frames': [],\n",
    "            'metadata': None,\n",
    "            'montage_path': None\n",
    "        }\n",
    "\n",
    "def batch_process_videos(video_ids, query, output_dir, frame_rate=0.1, max_frames=10, max_workers=3):\n",
    "    \"\"\"Process multiple videos in parallel.\n",
    "    Args:\n",
    "        video_ids: List of YouTube video IDs\n",
    "        query: Natural language query for content matching\n",
    "        output_dir: Directory to save output\n",
    "        frame_rate: Frames per second to extract\n",
    "        max_frames: Maximum number of frames to extract\n",
    "        max_workers: Maximum number of concurrent workers\n",
    "    Returns:\n",
    "        Dictionary with results for each video\n",
    "    \"\"\"\n",
    "    if not video_ids:\n",
    "        print(\"No video IDs provided\")\n",
    "        return {}\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = {}\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_video = {executor.submit(process_video, video_id, query, output_dir, frame_rate, max_frames): video_id for video_id in video_ids}\n",
    "        for future in tqdm(as_completed(future_to_video), total=len(video_ids), desc=\"Processing videos\"):\n",
    "            vid = future_to_video[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results[vid] = result\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {vid}: {str(e)}\")\n",
    "                results[vid] = {\n",
    "                    'video_id': vid,\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'frames': [],\n",
    "                    'metadata': None,\n",
    "                    'montage_path': None\n",
    "                }\n",
    "    return results\n",
    "\n",
    "def generate_batch_report(results, output_dir):\n",
    "    \"\"\"Generate a comprehensive report from batch processing results.\n",
    "    Args:\n",
    "        results: Dictionary with results for each video\n",
    "        output_dir: Directory to save the report\n",
    "    Returns:\n",
    "        Path to the generated report\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to include in report\")\n",
    "        return None\n",
    "    report_dir = os.path.join(output_dir, \"report\")\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    query = next(iter(results.values()))['metadata']['query'] if next(iter(results.values()))['metadata'] else \"Unknown\"\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_path = os.path.join(report_dir, f\"batch_report_{query.replace(' ', '_')}_{timestamp}.html\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Batch Processing Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "                h1, h2, h3 {{ color: #2c3e50; }}\n",
    "                .video-card {{ border: 1px solid #ddd; margin: 20px 0; padding: 15px; border-radius: 5px; }}\n",
    "                .video-header {{ display: flex; justify-content: space-between; align-items: center; }}\n",
    "                .success {{ color: green; }}\n",
    "                .failure {{ color: red; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                .montage {{ max-width: 100%; height: auto; margin: 10px 0; border: 1px solid #ddd; }}\n",
    "                .summary {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>YouTube Frame Extractor Batch Processing Report</h1>\n",
    "            <div class=\"summary\">\n",
    "                <h2>Summary</h2>\n",
    "                <p>Query: <strong>{query}</strong></p>\n",
    "                <p>Date: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "                <p>Total videos processed: {len(results)}</p>\n",
    "                <p>Successful: {sum(1 for r in results.values() if r['success'])}</p>\n",
    "                <p>Failed: {sum(1 for r in results.values() if not r['success'])}</p>\n",
    "            </div>\n",
    "        \"\"\")\n",
    "        f.write(\"\"\"\n",
    "            <h2>Results Overview</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Video ID</th>\n",
    "                    <th>Status</th>\n",
    "                    <th>Frames</th>\n",
    "                    <th>Top Score</th>\n",
    "                    <th>Avg Score</th>\n",
    "                </tr>\n",
    "        \"\"\")\n",
    "        for video_id, result in results.items():\n",
    "            status = \"Success\" if result['success'] else f\"Failed: {result.get('error', 'Unknown error')}\"\n",
    "            frame_count = len(result.get('frames', []))\n",
    "            top_score = result.get('metadata', {}).get('top_score', 'N/A')\n",
    "            avg_score = result.get('metadata', {}).get('average_score', 'N/A')\n",
    "            f.write(f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{video_id}</td>\n",
    "                    <td class=\"{'success' if result['success'] else 'failure'}\">{status}</td>\n",
    "                    <td>{frame_count}</td>\n",
    "                    <td>{top_score if top_score != 'N/A' else 'N/A'}</td>\n",
    "                    <td>{avg_score if avg_score != 'N/A' else 'N/A'}</td>\n",
    "                </tr>\n",
    "            \"\"\")\n",
    "        f.write(\"</table>\")\n",
    "        f.write(\"<h2>Detailed Results</h2>\")\n",
    "        for video_id, result in results.items():\n",
    "            f.write(f\"\"\"\n",
    "                <div class=\"video-card\">\n",
    "                    <div class=\"video-header\">\n",
    "                        <h3>Video: {video_id}</h3>\n",
    "                        <span class=\"{'success' if result['success'] else 'failure'}\">\n",
    "                            {\"Success\" if result['success'] else \"Failed\"}\n",
    "                        </span>\n",
    "                    </div>\n",
    "            \"\"\")\n",
    "            if result['success']:\n",
    "                f.write(f\"\"\"\n",
    "                    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/{video_id}\" \n",
    "                            frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; \n",
    "                            gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "                \"\"\")\n",
    "                if result.get('montage_path') and os.path.exists(result['montage_path']):\n",
    "                    montage_filename = os.path.basename(result['montage_path'])\n",
    "                    report_montage_path = os.path.join(report_dir, montage_filename)\n",
    "                    import shutil\n",
    "                    shutil.copy2(result['montage_path'], report_montage_path)\n",
    "                    f.write(f\"\"\"\n",
    "                        <h4>Top Matching Frames</h4>\n",
    "                        <img src=\"{montage_filename}\" alt=\"Frame montage\" class=\"montage\">\n",
    "                    \"\"\")\n",
    "                if result.get('metadata'):\n",
    "                    f.write(f\"\"\"\n",
    "                        <h4>Metadata</h4>\n",
    "                        <table>\n",
    "                            <tr><th>Frame Count</th><td>{result['metadata'].get('frame_count', 'N/A')}</td></tr>\n",
    "                            <tr><th>Top Score</th><td>{result['metadata'].get('top_score', 'N/A')}</td></tr>\n",
    "                            <tr><th>Average Score</th><td>{result['metadata'].get('average_score', 'N/A')}</td></tr>\n",
    "                        </table>\n",
    "                    \"\"\")\n",
    "            else:\n",
    "                f.write(f\"<p>Error: {result.get('error', 'Unknown error')}</p>\")\n",
    "            f.write(\"</div>\")\n",
    "        f.write(\"\"\"\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\")\n",
    "    return report_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a list of YouTube video IDs for batch processing\n",
    "batch_video_ids = [\n",
    "    \"nLrrOcXX2kw\",  # Nature documentary\n",
    "    \"eDiSYp_o_8Q\",  # Another nature video\n",
    "    \"dQw4w9WgXcQ\"   # Rick Astley (as a control)\n",
    "]\n",
    "\n",
    "# Define the query\n",
    "batch_query = \"animals in the wild\"\n",
    "\n",
    "# Create output directory for batch processing\n",
    "batch_output_dir = output_dir / \"batch_processing\"\n",
    "batch_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Batch processing code (currently commented out for safety)\n",
    "'''\n",
    "try:\n",
    "    print(f\"Starting batch processing of {len(batch_video_ids)} videos with query: '{batch_query}'\")\n",
    "    batch_results = batch_process_videos(\n",
    "        video_ids=batch_video_ids,\n",
    "        query=batch_query,\n",
    "        output_dir=str(batch_output_dir),\n",
    "        frame_rate=0.1,\n",
    "        max_frames=10,\n",
    "        max_workers=2\n",
    "    )\n",
    "    print(f\"Batch processing complete for {len(batch_results)} videos\")\n",
    "    report_path = generate_batch_report(batch_results, str(batch_output_dir))\n",
    "    if report_path:\n",
    "        print(f\"Generated batch report: {report_path}\")\n",
    "    print(\"\\nResults Summary:\")\n",
    "    for video_id, result in batch_results.items():\n",
    "        status = \"Success\" if result['success'] else \"Failed\"\n",
    "        frame_count = len(result.get('frames', []))\n",
    "        print(f\"- {video_id}: {status}, {frame_count} frames\")\n",
    "        if result['success'] and result.get('metadata'):\n",
    "            top_score = result['metadata'].get('top_score', 'N/A')\n",
    "            avg_score = result['metadata'].get('average_score', 'N/A')\n",
    "            print(f\"  Top score: {top_score:.3f}, Avg score: {avg_score:.3f}\")\n",
    "            if result.get('montage_path') and os.path.exists(result['montage_path']):\n",
    "                montage = Image.open(result['montage_path'])\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.imshow(montage)\n",
    "                plt.title(f\"Video {video_id} - Content Montage for '{batch_query}'\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in batch processing: {str(e)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Model Integration\n",
    "\n",
    "Here's how to integrate a custom analysis model with the YouTube Frame Extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CustomFrameAnalyzer:\n",
    "    \"\"\"A custom frame analyzer for demonstration purposes.\n",
    "    This example shows how to integrate a custom analysis model with the YouTube Frame Extractor framework.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Load any required models or resources here\n",
    "        self.model_loaded = True\n",
    "        print(\"Custom analyzer initialized\")\n",
    "    def analyze_frame(self, image):\n",
    "        \"\"\"Analyze a single frame.\n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "        channels = cv2.split(image) if len(image.shape) > 2 else [image]\n",
    "        channel_stats = []\n",
    "        for i, channel in enumerate(channels):\n",
    "            mean = np.mean(channel)\n",
    "            std = np.std(channel)\n",
    "            min_val = np.min(channel)\n",
    "            max_val = np.max(channel)\n",
    "            channel_stats.append({\n",
    "                'channel': i,\n",
    "                'mean': float(mean),\n",
    "                'std': float(std),\n",
    "                'min': int(min_val),\n",
    "                'max': int(max_val),\n",
    "                'contrast': float(max_val - min_val)\n",
    "            })\n",
    "        if len(channels) >= 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            brightness = float(np.mean(gray))\n",
    "            contrast = float(np.std(gray))\n",
    "        else:\n",
    "            brightness = float(np.mean(image))\n",
    "            contrast = float(np.std(image))\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        edge_density = float(np.count_nonzero(edges) / (edges.shape[0] * edges.shape[1]))\n",
    "        return {\n",
    "            'channel_stats': channel_stats,\n",
    "            'brightness': brightness,\n",
    "            'contrast': contrast,\n",
    "            'edge_density': edge_density,\n",
    "            'complexity_score': contrast * edge_density\n",
    "        }\n",
    "\n",
    "def analyze_frames_with_custom_model(frames):\n",
    "    \"\"\"Apply custom analysis to a list of frames.\n",
    "    Args:\n",
    "        frames: List of frame dictionaries\n",
    "    Returns:\n",
    "        List of frames with added analysis results\n",
    "    \"\"\"\n",
    "    if not frames:\n",
    "        return []\n",
    "    analyzer = CustomFrameAnalyzer()\n",
    "    analyzed_frames = []\n",
    "    for frame in frames:\n",
    "        if 'frame' in frame and frame['frame'] is not None:\n",
    "            image = frame['frame']\n",
    "        elif 'path' in frame and os.path.exists(frame['path']):\n",
    "            image = Image.open(frame['path'])\n",
    "        else:\n",
    "            analyzed_frames.append(frame)\n",
    "            continue\n",
    "        analysis_results = analyzer.analyze_frame(image)\n",
    "        analyzed_frame = frame.copy()\n",
    "        analyzed_frame['custom_analysis'] = analysis_results\n",
    "        analyzed_frames.append(analyzed_frame)\n",
    "    return analyzed_frames\n",
    "\n",
    "def visualize_custom_analysis(frames):\n",
    "    \"\"\"Visualize the results of custom frame analysis.\n",
    "    Args:\n",
    "        frames: List of frames with custom analysis results\n",
    "    \"\"\"\n",
    "    if not frames or 'custom_analysis' not in frames[0]:\n",
    "        print(\"No custom analysis results to visualize\")\n",
    "        return\n",
    "    times = [frame.get('time', i) for i, frame in enumerate(frames)]\n",
    "    brightness = [frame['custom_analysis']['brightness'] for frame in frames]\n",
    "    contrast = [frame['custom_analysis']['contrast'] for frame in frames]\n",
    "\n",
    "    # Plot brightness over time\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(times, brightness, '-o', label='Brightness')\n",
    "    plt.title(\"Brightness Over Time\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Brightness\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot contrast over time\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(times, contrast, '-o', label='Contrast', color='orange')\n",
    "    plt.title(\"Contrast Over Time\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Contrast\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot of brightness vs contrast\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(brightness, contrast, c=times, cmap='viridis')\n",
    "    plt.colorbar(label='Time (s)')\n",
    "    plt.title(\"Brightness vs Contrast\")\n",
    "    plt.xlabel(\"Brightness\")\n",
    "    plt.ylabel(\"Contrast\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run custom frame analysis on the extracted frames\n",
    "try:\n",
    "    print(\"Performing custom analysis on extracted frames...\")\n",
    "    custom_analyzed_frames = analyze_frames_with_custom_model(frames)\n",
    "    print(\"Custom analysis complete!\")\n",
    "    display_frames(custom_analyzed_frames, title=\"Frames with Custom Analysis\")\n",
    "    visualize_custom_analysis(custom_analyzed_frames)\n",
    "except Exception as e:\n",
    "    print(f\"Error during custom frame analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Finally, clean up any resources and summarize what we've learned in this advanced analysis session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cleanup resources\n",
    "try:\n",
    "    if 'browser_extractor' in locals() and browser_extractor._driver is not None:\n",
    "        browser_extractor._driver.quit()\n",
    "        print(\"Browser extractor cleaned up\")\n",
    "    for var in ['frames', 'multi_query_results', 'custom_analyzed_frames']:\n",
    "        if var in locals():\n",
    "            locals()[var] = None\n",
    "    print(\"Cleanup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered advanced analysis techniques using the YouTube Frame Extractor package:\n",
    "\n",
    "- **Advanced VLM Analysis:** Using multiple queries to compute similarity scores and create montages.\n",
    "- **Object Detection & Tracking:** Detecting objects in frames and tracking their occurrences across the video.\n",
    "- **Temporal Analysis:** Calculating differences between frames to detect scene changes and visualize content variation over time.\n",
    "- **Custom Model Integration:** Integrating a custom analyzer to compute image statistics such as brightness, contrast, and edge density, and visualizing these metrics.\n",
    "- **Advanced Batch Processing:** Demonstrating parallel processing of multiple videos (code provided but commented out).\n",
    "\n",
    "This advanced analysis notebook extends the basic capabilities by incorporating richer visualization and detailed analytics, making it a powerful tool for extracting actionable insights from video content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
